receivers:
  prometheus:
    config:
      scrape_configs:
      - job_name: "localhost"
        scrape_interval: 30s  # Conservative increase from 10s
        static_configs:
        # TODO: make this configurable
        - targets: ["localhost:2112"]
        # Do not relabel job and instance labels if existed.
        honor_labels: true
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: '^prometheus_.*'
            action: drop
          - source_labels: [ __name__ ]
            regex: '^process_.*'
            action: drop
          - source_labels: [ __name__ ]
            regex: '^go_.*'
            action: drop
          # Add these new filters for OpenTelemetry internal metrics
          - source_labels: [ __name__ ]
            regex: '^otel_scope_.*'
            action: drop

processors:
  attributes:
    actions:
      - action: insert
        key: team
        value: "REPLACE_ME_TEAM"
      - action: insert
        key: service_name
        value: "REPLACE_ME_SERVICE"

  batch:
    # batch metrics before sending to reduce API usage
    send_batch_max_size: 200
    send_batch_size: 200
    timeout: 15s  # Conservative increase from 5s, stays well below scrape interval

  memory_limiter:
    # drop metrics if memory usage gets too high
    check_interval: 1s
    limit_percentage: 65
    spike_limit_percentage: 20

  # automatically detect Cloud Run resource metadata
  resourcedetection:
    detectors: [env, gcp]

  resource:
    attributes:
    # Add instance_id as a resource attribute, so to avoid race conditions
    # between multiple otel sidecar instance uploading overlapping time series
    # to the same buckets.
    - key: service.instance.id
      from_attribute: faas.id
      action: upsert
    # The `gcp` resourcedetection processor sets `faas.name` to the name of the
    # Cloud Run service or the Cloud Run job.
    - from_attribute: faas.name
      # The googlemanagedprometheus exporter consumes `service.name` attribute
      # and set the `job` resource label to this value. (See
      # https://github.com/GoogleCloudPlatform/opentelemetry-operations-go/pull/764)
      key: "service.name"
      action: upsert

exporters:
  googlemanagedprometheus:
    project: "REPLACE_ME_PROJECT_ID"
    sending_queue:
      enabled: true
      # we are handling metrics for a single pod, no need to have
      # too many senders. this will also avoid out-of-order data.
      num_consumers: 1

extensions:
  health_check:

service:
  telemetry:
    logs:
      # We don't want to see scraper startup logging every
      # cold start.
      level: "error"
      # Stack trace is less useful and break lines.
      disable_stacktrace: true
      encoding: json

  extensions: [health_check]
  pipelines:
    metrics:
      receivers: [prometheus]
      processors: [batch, memory_limiter, resourcedetection, resource, attributes]
      exporters: [googlemanagedprometheus]
